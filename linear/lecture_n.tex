\chapter{Лекция N}
Вспомним: рассматривается система вида
\begin{equation}
    \dot{x}(t) = A(t) x(t) + B(t) u (t) + f(t),
\end{equation}
мы пытаемся перевести её из начального состояния $x^0$ в конечное $x^1$:
\begin{equation}
    x(t_0) = x^0 \to x(t_1) = x^1,
\end{equation}
минимизируя при этом норму управления в пространстве $L_2$:
\begin{equation}
    \mathcal{J}(u(\cdot)) = \norm{u(\cdot)}_{L_2} \to \inf\limits_{u(\cdot)}
\end{equation}

Данной постановке соответствует задача моментов
\begin{equation}\tag{ЗМ}
    \int_{t_0}^{t_1} H(t_1, t) u(t) dt = c,
\end{equation}
где $H(t_1, t) = \X(t_1, t) B(t)$.

Внимание! В этой лекции волевым усилием давайте во избежание дальнейшей путаницы начнём обозначать множество достижимости для задачи моментов за $\mathscr{H}_{\mu}^0$ вместо $\mathscr{X}_{\mu}^0$, чтобы точно не было никаких мыслей про то, что икс там и в обычном уравнении - это что-то очень похожее.

Пусть управление ограничено некоторым абстрактным значением $\mu$. Множеством достижимости для задачи моментов (ЗМ) при ограничении $\norm{u(\cdot)}_{L_2} \leqslant \mu$ договорились называть
\begin{equation}
    \H_{\mu}^0 = \left\{ \int_{t_0}^{t_1} H(t_1, t) u(t) dt \mid \norm{u(\cdot)}_{L_2} \leqslant \mu \right\}.
\end{equation}

Затем мы нашли опорную функцию множества достижимости этой ЗМ:
$$
\rho(l, \H_{\mu}^0) = \mu \norm{H'(t_1, \cdot)l}_{L_2} = \mu \sqrt{\scalar{l}{Wl}},
$$
где $W = W(t_1, t_0) = \int_{t_0}^{t_1} H(t_1, t) H'(t_1, t) dt$ --- матрица управляемости.

\section{Случай полной управляемости ($\abs{W} \neq 0$) --- геометрический смысл решения}
Если $\abs{W} \neq 0$ (что эквивалентно неравенству $W > 0$, $W = W'$), то оптимальное $\mu$ находится из соотношения
$$
\mu^* = \sqrt{\scalar{c}{W^{-1}c}},
$$
а оптимальное управление, соответственно, равно
$$
u^*(t) = H'(t_1,t) W^{-1}(t_1, t_0) c.
$$

Обсудим геометрическую интерпретацию полученного решения.

Предположим, что мы ищем управление $u(\cdot)$ \textit{приближённо} в классе кусочно~-постоянных функций:
$$
u(t) = \left\{
    \begin{aligned}
        \hat{u}^1, & \;\; t \in [t_0, \tau_1],\\
        \hat{u}^j, & \;\; t \in (\tau_{j-1}, \tau_{j}], \; j = \overline{2, N},
    \end{aligned}
\right.
$$
$\hat{u}^j \in \R^m$. Тогда каждое из рассматриваемых управлений определяется вектором
$$
u(\cdot) \leftrightarrow \begin{bmatrix}
    \hat{u}^1_1 \\ \ldots \\ \hat{u}^1_m \\ \vdots \\ \hat{u}^N_1 \\ \ldots \\ \hat{u}^N_m
\end{bmatrix} = \hat{u} \in \R^{m \times N}.
$$

Пусть для простоты $m = 1$ --- то есть управление скалярнозначное. Тогда кусочно-постоянная его аппроксимация (на $N$ отрезках сетки) будет выглядеть как
$$
\hat{u} = \begin{bmatrix}
    \hat{u}^1 \\ \vdots \\ \hat{u}^N
\end{bmatrix}
$$
Тогда наша (ЗМ) трансформируется в следующую задачу:
$$
\sum\limits_{j=1}^{N} \left[ \int\limits_{\tau_{j-1}}^{\tau_j} H(t_1, t) dt \right] \hat{u}^j = c
$$
При этом
$$
H(t_1, t) =
\begin{bmatrix}
    - h_1(t_1, t) -\\
    - h_2(t_1, t) -\\
    \ldots \\
    - h_n(t_1, t) -\\
\end{bmatrix}
\in \R^{n \times m}.
$$

Вспомним, что мы для простоты взяли $m = 1$ и обозначим
$$
\hat{g}^j_i = \int\limits_{\tau_{j-1}}^{\tau_j} h_i(t_1, t) dt,
$$
то есть
$$
\int\limits_{\tau_{j-1}}^{\tau_j} H(t_1, t) dt = [\hat{g}^j_1, \ldots, \hat{g}^j_n]'.
$$

Подставим $c = [c_1  \ldots c_n]'$ $\thus$ получаем СЛАУ относительно $\hat{u}^j$:
$$
\sum\limits_{j=1}^{N} \hat{g}^j_i \hat{u}^j = c_i, \; i = \overline{1, n}.
$$

Перепишем эту систему в другом виде.
Сгруппируем компоненты $\hat{g}^j_i$ по координатам $i$, обозначим:
$$
\hat{g}_i = [\hat{g}^1_i, \ldots,  \hat{g}^N_i]'
$$
\textit{(если уже успели запутаться: $i \leqslant n = \dim(x)$, а $N$ --- количество отрезков, которые мы выбрали для построения кусочно-постоянной аппроксимации управления)}.

Тогда наша СЛАУ перепишется в следующем виде \textit{(уже без $i = \overline{1, n}$)}.
\begin{equation} \tag{$\star$}
\left\{
    \begin{aligned}
        & \scalar{\hat{g}_1}{\hat{u}} = c_1, \\
        & \ldots\\
        & \scalar{\hat{g}_n}{\hat{u}} = c_n,
    \end{aligned}
\right.
\end{equation}
и минимизацию мы будем проводить для функционала (вводим равномерную сетку $\Delta t = (t_1 - t_0) / N$)
$$
\mathcal{J}(u(\cdot)) = \sqrt{\Delta t} \norm{\hat{u}} \to \min.
$$

Пусть теперь $n = 2, N = 3$.

Поскольку уравнение $\scalar{g}{u} = c$ задаёт гиперплоскость (относительно $u$, с нормалью $g$ и вектором сдвига $c$), получаемая нами система уравнений
$$
\left\{
    \begin{aligned}
        & \scalar{\hat{g}_1}{\hat{u}} = c_1, \\
        & \scalar{\hat{g}_2}{\hat{u}} = c_2,
    \end{aligned}
\right.
$$
задаёт пересечение двух гиперплоскостей --- обозначим эту прямую за $L$, $L \bot \L \{\hat{g}_1, \hat{g}_2\}$. Мы ищем управление $\hat{u} \in L$; при этом оно должно быть оптимальным:
$$
\hat{u}^* \in L \colon \; \norm{\hat{u}^*} \leqslant \norm{\hat{u}} \forall \hat{u} \in L.
$$

Ясно, что
$$
\hat{u}^* \in \L \{\hat{g}_1, \hat{g}_2\} \cap L;
$$
на каждом отрезке аппроксимации (напомним: мы взяли скалярное $u(t) \in \R$ и приблизили его тремя кусочно-постоянными отрезками времени, N=3)
$$
\hat{u}^{*j} = \alpha_1 \hat{g}_1^j + \alpha_2 \hat{g}_2^j, \; j = 1, 2, 3.
$$

Что мы знаем про $\hat{g}_1^j$? Из теоремы о среднем
$$
\hat{g}_i^j = h_i(t_1, \xi_i^j) \Delta t, \; \xi_i^j \in (\tau_{j-1}, \tau_{j}),
$$
то есть
$$
\hat{u}^{*}(\tau_{j-1}) = \hat{u}^{*j} =
\begin{bmatrix}
    h_1(t_1, \xi_1^j), \\
    h_2(t_1, \xi_2^j)    
\end{bmatrix}'
\begin{pmatrix}
    \alpha_1\\
    \alpha_2
\end{pmatrix},
\; j = 1, \ldots, N.
$$

Устремим теперь $N \to \infty$. Тогда $\tau_{j-1}, \tau_{j} \to t$, $t \in [\tau_{j-1}, \tau_{j}]$, $\xi_i^j \to t$, и
$$
u^*(t) = 
\begin{bmatrix}
    h_1(t_1, t, \\
    h_2(t_1, t)    
\end{bmatrix}'
\begin{pmatrix}
    \alpha_1\\
    \alpha_2
\end{pmatrix}
= H'(t_1, t) \alpha
= \alpha_1 h_1(t_1, t) + \alpha_2 h_2(t_1, t).
$$
Подставим это управление в (ЗМ), получим:
$$
\   int_{t_0}^{t_1} H(t_1, t) H'(t_1, t) \alpha dt = c \thus alpha = W^{-1} c,
$$
что совпадает с нашим решением, которое было получено раньше.

Вернёмся к \underline{линейной независимости}.
Система $(\star)$ разрешима, если её матрица Грамма не вырождена:
$$
\Gamma = \left[ \scalar{\hat{g}_k}{\hat{g}_l} \right] =
\begin{bmatrix}
    \scalar{\hat{g}_1}{\hat{g}_1} & \scalar{\hat{g}_1}{\hat{g}_2} \\
    \scalar{\hat{g}_2}{\hat{g}_1} & \scalar{\hat{g}_2}{\hat{g}_2} 
\end{bmatrix}
$$
Каждое скалярное произведение в этой матрице
$$
\scalar{\hat{g}_k}{\hat{g}_l} = (\Delta t)^2 \cdot \scalar{
\begin{bmatrix}
    h_k(t_1, \xi_k^1) \\
    \ldots \\
    h_k(t_1, \xi_k^N)
\end{bmatrix}
}{
    \begin{bmatrix}
        h_l(t_1, \xi_l^1) \\
        \ldots \\
        h_l(t_1, \xi_l^N)
    \end{bmatrix}
} = \Delta t^2 \cdot <\ldots>.
$$

Верно, что $\abs{\Gamma} \neq 0 \Leftrightarrow \abs{\tilde{\Gamma}} \neq 0$, где
$$
\tilde{\Gamma} = 
\begin{bmatrix}
    \dfrac{\scalar{\hat{g}_k}{\hat{g}_l}}{\Delta t}
\end{bmatrix},
$$
и
$$
\dfrac{\scalar{\hat{g}_k}{\hat{g}_l}}{\Delta t} = \Delta t \cdot <\ldots> =
\sum\limits_{j = 1}^{N} h_k (t_1, \xi_k^j) h_l (t_1, \xi_l^j) \to \{ N \to \infty, m = 1 \} \to \int\limits_{t_0}^{t_1} h_k (t_1, t) h_l(t_1, t).
$$

Вспоминаем про $W(t_1, t_0) = (w_{kl}(t_1, t_0))$, следовательно,
$$
\int\limits_{t_0}^{t_1} h_k (t_1, t) h_l(t_1, t) = w_{kl}(t_1, t_0) =
\int\limits_{t_0}^{t_k} \scalar{h_k(t_1, t)}{ h_l(t_1, t)} dt = \scalar{h_k(t_1, \cdot)}{h_k(t_1, \cdot)}_{L_2}.
$$

Таким образом, $W$ --- матрица Грама функций
$$
h_1(t_1, \cdot), \ldots, h_n(t_1, \cdot) \in \R^{1 \times m}.
$$